import Documentation from "df/docs/layouts/documentation";

export default ({ children }) => (
  <Documentation title="Command line interface">{children}</Documentation>
);

## Installation

Dataform can be installed using <a target="_blank" href="https://www.npmjs.com/get-npm">NPM</a>:

```bash
npm i -g @dataform/cli
```

## Create a new project

To create a new `bigquery`, `postgres`, `redshift`, or `snowflake` project in the `new_project` directory, run the respective command:

```bash
dataform init bigquery new_project --gcloud-project-id<your-google-cloud-project-id>
- or -
dataform init postgres new_project
- or -
dataform init redshift new_project
- or -
dataform init snowflake new_project
```

### Project structure

Change directory into the newly-created `new_project` directory and take a look at your newly created project files:

```bash
cd new_project
ls
```

You should see the following structure:

```bash
project-dir
├── definitions
├── includes
├── package.json
└── dataform.json
```

## Define a table

The `definitions/` directory should be used for files that define [tables](/tables), [assertions](/assertions), and [operations](/operations).

To create a new table, create a new file `definitions/example.sql`:

```js
SELECT 1 AS test
```

## Compile your code

To check that your Dataform code compiles, run the `compile` command at the root of your project directory to get JSON output of the compiled project:

```bash
dataform compile
```

You should see output similar to the following:

```json
{
    "tables": [
        {
            "name": "example",
            "type": "view",
            "target": {
                "schema": "dataform",
                "name": "example"
            },
            "query": "SELECT 1 AS test",
            "disabled": false,
            "fileName": "definitions/example.sql"
        }
    ],
    ...
}
```

## Create a credentials file

Dataform requires a credentials file in order to connect to your warehouse. Run the `init-creds` command and Dataform will guide you through credentials file creation:

```bash
dataform init-creds bigquery
- or -
dataform init-creds postgres
- or -
dataform init-creds redshift
- or -
dataform init-creds snowflake
```

A `.df-credentials.json` file will be written to disk containing your provided details.

Check out our [data warehouse setup guide](/platform_guides/set_up_datawarehouse) if you need help with the `init-creds` wizard.

<div className="pt-callout pt-icon-info-sign pt-intent-warning" markdown="1">
  If using a source control system, we strongly recommend that you do not commit the{" "}
  <code>.df-credentials.json</code> file to your repository in order to protect these access
  credentials.
</div>

## Run your code

In order to run your code, Dataform needs to access your data warehouse in order to determine its current state and tailor the resulting
SQL accordingly. If you'd like to see the final SQL that Dataform will run on your warehouse without actually running it, you can perform a dry run:

```bash
dataform run --dry-run
```

You should see something similar to the following:

```json
{
      ...
      "nodes": [
        {
            "name": "example",
            "tasks": [
                {
                    "type": "statement",
                    "statement": "create or replace view \"dataform\".\"example\" as SELECT 1 AS test;"
                }
            ],
            "type": "table",
            "target": {
                "schema": "dataform",
                "name": "example"
            },
            "tableType": "view"
        }
    ],
    ...
}
```

Removing the `--dry-run` option will result in the SQL being run in your warehouse:

```bash
dataform run
```

The `run` command's output will now include the run's execution status, including any errors encountered during the run.
